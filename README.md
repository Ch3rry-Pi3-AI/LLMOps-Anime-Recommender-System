# ğŸŒ¸ **LLMOps Anime Recommender System â€” Project Overview**

This repository presents a **complete LLMOps workflow** for an **anime recommendation system**, covering every layer from **data processing and embedding generation** to **LLM reasoning**, **containerisation**, and **cloud deployment** on **Google Cloud Platform (GCP)** using **Docker** and **Kubernetes**.

<p align="center">
  <img src="img/streamlit/streamlit_app.gif" alt="Streamlit App Demo" width="100%">
</p>

## ğŸ§© **Grouped Stages**

|     #     | Stage                            | Description                                                                                                                   |
| :-------: | :------------------------------- | :---------------------------------------------------------------------------------------------------------------------------- |
|   **00**  | **Project Setup**                | Established the base VS Code structure, environment setup, and configuration files.                                           |
| **01â€“05** | **Core LLM Logic**               | Built the data loader, vector store, prompt template, and recommender class, then unified them through a modular pipeline.    |
|   **06**  | **Streamlit Application**        | Developed a responsive front end that allows users to generate personalised anime recommendations.                            |
|   **07**  | **Containerisation & Manifests** | Authored the Dockerfile and Kubernetes YAML manifests for scalable deployment.                                                |
|   **08**  | **GCP VM & Docker Setup**        | Configured a GCP Virtual Machine and installed Docker Engine for container management.                                        |
| **09â€“11** | **Cluster & Deployment**         | Installed Minikube and kubectl, configured firewall rules, deployed the app on Kubernetes, and exposed it via an external IP. |
|   **12**  | **Grafana Cloud Monitoring**     | Integrated Grafana Cloud for visualising metrics, system health, and performance trends.                                      |

## ğŸ—‚ï¸ **Project Structure**

```text
llmops_anime_recommender_system/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ data/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ build_pipeline.py
â”‚   â””â”€â”€ recommendation_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ prompt_template.py
â”‚   â””â”€â”€ recommender.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ img/
â”‚   â””â”€â”€ streamlit/streamlit_app.gif
â”œâ”€â”€ Dockerfile                 # Defines container image
â”œâ”€â”€ llmops-k8s.yaml            # Kubernetes Deployment + Service manifest
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ uv.lock
â””â”€â”€ README.md
```

## ğŸš€ **Summary**

The **LLMOps Anime Recommender System** demonstrates how to take a **retrieval-augmented generation (RAG)** workflow from prototype to production.
It combines **LLM reasoning**, **vector-based retrieval**, and **prompt engineering** within a robust **MLOps pipeline** deployed on **Kubernetes**, forming a scalable foundation for future **LLMOps-driven applications**.

