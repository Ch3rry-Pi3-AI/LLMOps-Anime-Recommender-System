# ğŸ’¬ **Prompt Engineering Integration â€” LLMOps Anime Recommender System**

This stage adds the **prompt engineering component** to the **LLMOps Anime Recommender System**.
The `get_anime_prompt()` function introduces a structured **LangChain `PromptTemplate`** that guides the LLM in generating high-quality, context-aware anime recommendations.
Combined with the vector store and data loader, this marks the transition from **data preparation** to **LLM-driven retrieval and reasoning**.

## ğŸ—‚ï¸ **Project Structure (Updated)**

```text
llmops_anime_recommender_system/
â”œâ”€â”€ .env                             # ğŸ”‘ API keys (Groq & Hugging Face)
â”œâ”€â”€ .gitignore                       # ğŸš« Git ignore rules
â”œâ”€â”€ .python-version                  # ğŸ Python version pin for consistency
â”œâ”€â”€ app/                             # ğŸ¨ Streamlit application (to be developed)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                    # âš™ï¸ Loads environment variables and model configuration
â”œâ”€â”€ data/                            # ğŸ“Š Contains raw and processed anime datasets
â”œâ”€â”€ pipeline/                        # ğŸ” Placeholder for workflow scripts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py               # ğŸ“¥ Loads and preprocesses the anime dataset
â”‚   â”œâ”€â”€ vector_store_builder.py      # ğŸ§  Builds and loads the Chroma vector store
â”‚   â””â”€â”€ prompt_template.py           # ğŸ’¬ Defines the LLM prompt structure for recommendations
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_exception.py          # Unified error handling
â”‚   â””â”€â”€ logger.py                    # Centralised logging setup
â”œâ”€â”€ pyproject.toml                   # ğŸ§© Project metadata and uv configuration
â”œâ”€â”€ requirements.txt                 # ğŸ“¦ Dependencies
â”œâ”€â”€ setup.py                         # ğŸ”§ Editable install support
â”œâ”€â”€ uv.lock                          # ğŸ”’ Dependency lock file
â””â”€â”€ README.md                        # ğŸ“– Documentation (you are here)
```

## âš™ï¸ **Overview of `prompt_template.py`**

The **`get_anime_prompt()`** function defines a **LangChain `PromptTemplate`** that ensures consistent, structured responses from the LLM.
It sets clear behavioural instructions for the model to act as an *expert anime recommender* and respond within a specific, well-formatted structure.

### Key Features

1. **Consistent Response Format** â€” Generates exactly three recommendations, each with a title, short plot summary, and rationale.
2. **Context-Aware Reasoning** â€” Incorporates vector-retrieved context to make the responses relevant to user preferences.
3. **Controlled Output** â€” Instructs the LLM not to fabricate information if insufficient context is available.
4. **Reusability** â€” Encapsulated in a single callable function for clean integration with the LangChain pipeline.

### Example Usage

```python
from src.prompt_template import get_anime_prompt

prompt = get_anime_prompt()
formatted_prompt = prompt.format(
    context="Top-rated anime featuring adventure and strong female leads.",
    question="Can you suggest anime similar to Violet Evergarden?"
)

print(formatted_prompt)
```

### Output Example

```
You are an expert anime recommender. Your job is to help users find the perfect anime...

Context:
Top-rated anime featuring adventure and strong female leads.

User's question:
Can you suggest anime similar to Violet Evergarden?

Your well-structured response:
```

The formatted prompt ensures that downstream LLM responses are both **coherent** and **contextually grounded**.

## âœ… **In Summary**

This stage completes the **retrieval and prompt engineering foundation** of the system:

* Adds `get_anime_prompt()` for structured, consistent LLM communication.
* Connects the **data**, **vector store**, and **prompt** layers into a unified retrieval-augmented workflow.
* Prepares the system for the next phase â€” **end-to-end recommendation inference and Streamlit integration**.
