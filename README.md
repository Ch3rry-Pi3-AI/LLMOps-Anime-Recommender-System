# ğŸŒ¸ **LLMOps Anime Recommender System â€” Project Overview**

This project implements a complete **LLMOps workflow** for an **anime recommender system**, combining data preparation, vector storage, prompt engineering, LLM reasoning, and full deployment on **Google Cloud Platform (GCP)** using **Docker** and **Kubernetes**.

<p align="center">
  <img src="img/streamlit/streamlit_app.gif" alt="Streamlit App Demo" width="100%">
</p>

## ğŸ§© **Grouped Stages**

* **00 â€” Project Setup**
  Established the base VS Code structure, environment setup, and configuration files.

* **01â€“05 â€” Core LLM Logic**
  Implemented the data loader, vector store, prompt template, recommender class, and integrated them through the pipeline.

* **06 â€” Streamlit Application**
  Built the front-end interface for generating recommendations interactively.

* **07 â€” Containerisation and Manifests**
  Created the Dockerfile and Kubernetes deployment YAML files.

* **08 â€” GCP VM and Docker Setup**
  Configured a GCP Virtual Machine and installed Docker Engine.

* **09â€“11 â€” Cluster and Deployment**
  Installed Minikube and kubectl, created firewall rules, deployed the application on Kubernetes, and exposed it via an external IP.

* **12 â€” Grafana Cloud Monitoring**
  Integrated Grafana for system monitoring and performance tracking.

## ğŸ—‚ï¸ **Project Structure**

```text
llmops_anime_recommender_system/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ data/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ build_pipeline.py
â”‚   â””â”€â”€ recommendation_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ prompt_template.py
â”‚   â””â”€â”€ recommender.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ img/
â”‚   â””â”€â”€ streamlit/streamlit_app.gif
â”œâ”€â”€ Dockerfile                 # Container image definition
â”œâ”€â”€ llmops-k8s.yaml            # Kubernetes Deployment + Service manifest
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ uv.lock
â””â”€â”€ README.md
```

## âœ… **Summary**

This project demonstrates how to take a **retrieval-augmented generation (RAG)** system from concept to production deployment. It combines **LLM-driven recommendation logic**, **container orchestration**, and **cloud deployment** in a single cohesive MLOps pipeline â€” forming a complete foundation for future scalable LLMOps applications.
